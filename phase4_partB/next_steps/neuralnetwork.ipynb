{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e544da31-acbc-492c-8257-4cffe3bd23f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from textblob import TextBlob\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "from statistics import mean\n",
    "from sklearn.utils import resample\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import scipy.sparse as sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c84a0662-24c3-45be-b3eb-c2058f6af980",
   "metadata": {},
   "outputs": [],
   "source": [
    "downsampled = pd.read_csv('./downsampled_fixed_spelling.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "08f25ebc-d8d8-4b90-a9ef-209a5c0d3e46",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Vectorizing text data: 15000it [00:00, 31369.54it/s]           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 359 ms\n",
      "Wall time: 549 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#attempt a neural network approach\n",
    "np.random.seed(10)\n",
    "total_docs = 1500\n",
    "X = downsampled['review_text']\n",
    "y = downsampled['rating']-1#because nn\n",
    "\n",
    "def track_progress(progress):\n",
    "    return tqdm(progress, total=total_docs, desc=\"Vectorizing text data\")\n",
    "\n",
    "#only the 1500 most frequent tokens are used\n",
    "tfidfconverter = CountVectorizer(max_features=1500, min_df=5, max_df=0.7)\n",
    "vectorized_data = tfidfconverter.fit_transform(track_progress(X))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(vectorized_data, y, test_size=0.2, random_state=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75578353-e21c-4bad-9fa3-c2cf57f40789",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "11ced26b-7b9e-4eac-8282-0ebb3ccf34d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.utils.data.dataloader.DataLoader object at 0x000001BE02C14A90>\n",
      "epoch: 0\n",
      "epoch: 1\n",
      "epoch: 2\n",
      "epoch: 3\n",
      "epoch: 4\n",
      "CPU times: total: 3h 5min 39s\n",
      "Wall time: 25min 34s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "class CNNClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, num_filters, filter_sizes, hidden_dim, output_dim, dropout):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Embedding layer\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        # Convolutional layers\n",
    "        self.convs = nn.ModuleList([\n",
    "            nn.Conv2d(in_channels=1, out_channels=num_filters, kernel_size=(fs, embedding_dim)) \n",
    "            for fs in filter_sizes\n",
    "        ])\n",
    "        \n",
    "        # Fully-connected layers\n",
    "        self.fc = nn.Linear(len(filter_sizes) * num_filters, hidden_dim)\n",
    "        self.output = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "        # Dropout layer\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, text):\n",
    "        # text shape: [batch_size, seq_len]\n",
    "        embedded = self.embedding(text)\n",
    "        # embedded shape: [batch_size, seq_len, embedding_dim]\n",
    "        \n",
    "        # Add channel dimension for convolution\n",
    "        embedded = embedded.unsqueeze(1)\n",
    "        # embedded shape: [batch_size, 1, seq_len, embedding_dim]\n",
    "        \n",
    "        # Apply convolutional layers\n",
    "        conved = [nn.functional.relu(conv(embedded)).squeeze(3) for conv in self.convs]\n",
    "        # conved[i] shape: [batch_size, num_filters, seq_len - filter_sizes[i] + 1]\n",
    "        \n",
    "        # Apply max pooling\n",
    "        pooled = [nn.functional.max_pool1d(conv, conv.shape[2]).squeeze(2) for conv in conved]\n",
    "        # pooled[i] shape: [batch_size, num_filters]\n",
    "        \n",
    "        # Concatenate pooled features\n",
    "        cat = self.dropout(torch.cat(pooled, dim=1))\n",
    "        # cat shape: [batch_size, len(filter_sizes) * num_filters]\n",
    "        \n",
    "        # Apply fully-connected layers\n",
    "        hidden = nn.functional.relu(self.fc(cat))\n",
    "        # hidden shape: [batch_size, hidden_dim]\n",
    "        \n",
    "        # Apply output layer\n",
    "        output = self.output(hidden)\n",
    "        # output shape: [batch_size, output_dim]\n",
    "        \n",
    "        return output\n",
    "\n",
    "# Define hyperparameters\n",
    "vocab_size = 10000\n",
    "embedding_dim = 100\n",
    "num_filters = 100\n",
    "filter_sizes = [3, 4, 5]\n",
    "hidden_dim = 256\n",
    "output_dim = 5\n",
    "dropout = 0.5\n",
    "num_epochs =5\n",
    "# Initialize model\n",
    "model = CNNClassifier(vocab_size, embedding_dim, num_filters, filter_sizes, hidden_dim, output_dim, dropout)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "# dataset = sp.hstack([X_train, sp.coo_matrix(y_train.to_numpy()).T]).toarray()\n",
    "inputs = torch.tensor(X_train.toarray())\n",
    "targets = torch.tensor(y_train.values)\n",
    "dataset = TensorDataset(inputs, targets)\n",
    "train_loader = DataLoader(dataset)\n",
    "\n",
    "# Train model\n",
    "for epoch in range(num_epochs):\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        # Clear gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f\"epoch: {epoch}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "de08497a-0aea-4b02-921f-e523eb67474a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.24      0.34      1388\n",
      "           1       0.01      0.71      0.02         7\n",
      "           2       0.03      0.14      0.06       143\n",
      "           3       0.00      0.00      0.00         2\n",
      "           4       0.64      0.27      0.38      1460\n",
      "\n",
      "    accuracy                           0.25      3000\n",
      "   macro avg       0.25      0.27      0.16      3000\n",
      "weighted avg       0.58      0.25      0.35      3000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "outputs = model(torch.LongTensor(X_test.todense()))\n",
    "_, y_pred = torch.max(outputs.data, 1)\n",
    "print(classification_report(y_pred, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b17bd387-8824-40fa-821c-38336c460280",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
